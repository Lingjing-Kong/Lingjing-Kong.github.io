<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Publications | Lingjing Kong</title>
<meta name=keywords content><meta name=description content="

  * indicates co-first authorship.


Learning Discrete Concepts in Latent Hierarchical Models
Lingjing Kong, Guangyi Chen, Biwei Huang, Eric Xing, Yuejie Chi & Kun Zhang.
NeurIPS 2024"><meta name=author content="Lingjing Kong"><link rel=canonical href=https://Lingjing-Kong.github.io/publications/><link crossorigin=anonymous href=/assets/css/stylesheet.46f4266ddd08f1e9d7c4353892a43520d7dbcfe6a0383857770de97990d1b2b5.css integrity="sha256-RvQmbd0I8enXxDU4kqQ1INfbz+agODhXdw3peZDRsrU=" rel="preload stylesheet" as=style><link rel=icon href=https://Lingjing-Kong.github.io/images/scslogo_no_outline.png><link rel=icon type=image/png sizes=16x16 href=https://Lingjing-Kong.github.io/images/scslogo_no_outline.png><link rel=icon type=image/png sizes=32x32 href=https://Lingjing-Kong.github.io/images/scslogo_no_outline.png><link rel=apple-touch-icon href=https://Lingjing-Kong.github.io/images/scslogo_no_outline.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Lingjing-Kong.github.io/publications/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Publications"><meta property="og:description" content="

  * indicates co-first authorship.


Learning Discrete Concepts in Latent Hierarchical Models
Lingjing Kong, Guangyi Chen, Biwei Huang, Eric Xing, Yuejie Chi & Kun Zhang.
NeurIPS 2024"><meta property="og:type" content="article"><meta property="og:url" content="https://Lingjing-Kong.github.io/publications/"><meta property="article:section" content><meta name=twitter:card content="summary"><meta name=twitter:title content="Publications"><meta name=twitter:description content="

  * indicates co-first authorship.


Learning Discrete Concepts in Latent Hierarchical Models
Lingjing Kong, Guangyi Chen, Biwei Huang, Eric Xing, Yuejie Chi & Kun Zhang.
NeurIPS 2024"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Publications","item":"https://Lingjing-Kong.github.io/publications/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Publications","name":"Publications","description":" * indicates co-first authorship. Learning Discrete Concepts in Latent Hierarchical Models\nLingjing Kong, Guangyi Chen, Biwei Huang, Eric Xing, Yuejie Chi \u0026amp; Kun Zhang.\nNeurIPS 2024\n","keywords":[],"articleBody":" * indicates co-first authorship. Learning Discrete Concepts in Latent Hierarchical Models\nLingjing Kong, Guangyi Chen, Biwei Huang, Eric Xing, Yuejie Chi \u0026 Kun Zhang.\nNeurIPS 2024\nTowards Understanding Extrapolation: a Causal Lens\nLingjing Kong*, Guangyi Chen*, Eric Xing \u0026 Kun Zhang.\nNeurIPS 2024\nCounterfactual Generation with Identifiability Guarantees\nHanqi Yan*, Lingjing Kong*, Lin Gui, Yuejie Chi, Eric Xing, Yulan He \u0026 Kun Zhang.\nNeurIPS 2023\nIdentification of Nonlinear Latent Hierarchical Causal Models\nLingjing Kong, Biwei Huang, Feng Xie, Eric Xing, Yuejie Chi \u0026 Kun Zhang.\nNeurIPS 2023\nUnderstanding Masked Autoencoders via Hierarchical Latent Variable Models\nLingjing Kong*, Martin Ma*, Guangyi Chen, Eric Xing, Yuejie Chi, Louis-Philippe Morency \u0026 Kun Zhang.\nCVPR 2023, Highlight: top 2.6%\nMulti-domain Image Generation and Translation with Identifiability Guarantees\nShaoan Xie, Lingjing Kong, Mingming Gong \u0026 Kun Zhang.\nICLR 2023, Spotlight: top 3.8%\nPartial Identifiability for Domain Adaptation\nLingjing Kong, Shaoan Xie, Weiran Yao, Yujia Zheng, Guangyi Chen, Petar Stojanov, Victor Akinwande \u0026 Kun Zhang.\nICML 2022\nConsensus Control for Decentralized Deep Learning\nLingjing Kong*, Tao Lin*, Anastasia Koloskova, Martin Jaggi \u0026 Sebastian U. Stich.\nICML 2021\nEnsemble Distillation for Robust Model Fusion in Federated Learning\nTao Lin*, Lingjing Kong*, Sebastian U. Stich \u0026 Martin Jaggi.\nNeurIPS 2020\nExtrapolation for Large Batch Training in Deep Learning\nTao Lin*, Lingjing Kong*, Sebastian U. Stich \u0026 Martin Jaggi.\nICML 2020\nSelf-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems\nFei Mi, Wanhao Zhou, Lingjing Kong, Fengyu Cai, Minlie Huang \u0026 Boi Faltings.\nEMNLP 2021\nGeneralized Class Incremental Learning\nFei Mi*, Lingjing Kong*, Tao Lin, Kaicheng Yu \u0026 Boi Faltings.\nCVPR 2020 Workshop\nPower Consumption Evaluation in High Speed Visible Light Communication Systems\nLingjing Kong, Cheng Chen, Yunlu Wang \u0026 Harald Hass.\nIEEE Globecom Conference 2018\n","wordCount":"288","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Lingjing Kong"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://Lingjing-Kong.github.io/publications/"},"publisher":{"@type":"Organization","name":"Lingjing Kong","logo":{"@type":"ImageObject","url":"https://Lingjing-Kong.github.io/images/scslogo_no_outline.png"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity=sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity=sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity=sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{equation*}",right:"\\end{equation*}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{align*}",right:"\\end{align*}",display:!0},{left:"\\begin{alignat}",right:"\\end{alignat}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0},{left:"\\begin{CD}",right:"\\end{CD}",display:!0}],throwOnError:!1})})</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://Lingjing-Kong.github.io/ accesskey=h title="Lingjing Kong"><img src=https://Lingjing-Kong.github.io/images/scslogo_no_outline.png alt aria-label=logo height=18 width=18>Lingjing Kong</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://Lingjing-Kong.github.io/publications/ title=Publications><span class=active>Publications</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Publications</h1></header><div class=post-content><p style=font-size:.9em;color:#666;margin-bottom:1em><em>* indicates co-first authorship.</em></p><p><span style=color:#006400><a href=https://arxiv.org/abs/2406.00519 target=_blank>Learning Discrete Concepts in Latent Hierarchical Models</a></span><br><strong>Lingjing Kong</strong>, Guangyi Chen, Biwei Huang, Eric Xing, Yuejie Chi & Kun Zhang.<br><em>NeurIPS 2024</em></p><p><span style=color:#006400><a href=https://arxiv.org/abs/2501.09163 target=_blank>Towards Understanding Extrapolation: a Causal Lens</a></span><br><strong>Lingjing Kong</strong>*, Guangyi Chen*, Eric Xing & Kun Zhang.<br><em>NeurIPS 2024</em></p><p><span style=color:#006400><a href=https://arxiv.org/abs/2402.15309 target=_blank>Counterfactual Generation with Identifiability Guarantees</a></span><br>Hanqi Yan*, <strong>Lingjing Kong</strong>*, Lin Gui, Yuejie Chi, Eric Xing, Yulan He & Kun Zhang.<br><em>NeurIPS 2023</em></p><p><span style=color:#006400><a href=https://arxiv.org/abs/2306.07916 target=_blank>Identification of Nonlinear Latent Hierarchical Causal Models</a></span><br><strong>Lingjing Kong</strong>, Biwei Huang, Feng Xie, Eric Xing, Yuejie Chi & Kun Zhang.<br><em>NeurIPS 2023</em></p><p><span style=color:#006400><a href=https://arxiv.org/abs/2306.04898 target=_blank>Understanding Masked Autoencoders via Hierarchical Latent Variable Models</a></span><br><strong>Lingjing Kong</strong>*, Martin Ma*, Guangyi Chen, Eric Xing, Yuejie Chi, Louis-Philippe Morency & Kun Zhang.<br><em>CVPR 2023</em>, <em>Highlight: top 2.6%</em></p><p><span style=color:#006400><a href="https://openreview.net/pdf?id=U2g8OGONA_V" target=_blank>Multi-domain Image Generation and Translation with Identifiability Guarantees</a></span><br>Shaoan Xie, <strong>Lingjing Kong</strong>, Mingming Gong & Kun Zhang.<br><em>ICLR 2023</em>, <em>Spotlight: top 3.8%</em></p><p><span style=color:#006400><a href=https://arxiv.org/abs/2306.06510 target=_blank>Partial Identifiability for Domain Adaptation</a></span><br><strong>Lingjing Kong</strong>, Shaoan Xie, Weiran Yao, Yujia Zheng, Guangyi Chen, Petar Stojanov, Victor Akinwande & Kun Zhang.<br><em>ICML 2022</em></p><p><span style=color:#006400><a href=https://arxiv.org/abs/2102.04828 target=_blank>Consensus Control for Decentralized Deep Learning</a></span><br><strong>Lingjing Kong</strong>*, Tao Lin*, Anastasia Koloskova, Martin Jaggi & Sebastian U. Stich.<br><em>ICML 2021</em></p><p><span style=color:#006400><a href=https://arxiv.org/abs/2006.07242 target=_blank>Ensemble Distillation for Robust Model Fusion in Federated Learning</a></span><br>Tao Lin*, <strong>Lingjing Kong</strong>*, Sebastian U. Stich & Martin Jaggi.<br><em>NeurIPS 2020</em></p><p><span style=color:#006400><a href=https://arxiv.org/abs/2006.05720 target=_blank>Extrapolation for Large Batch Training in Deep Learning</a></span><br>Tao Lin*, <strong>Lingjing Kong</strong>*, Sebastian U. Stich & Martin Jaggi.<br><em>ICML 2020</em></p><p><span style=color:#006400><a href=https://arxiv.org/abs/2108.12589 target=_blank>Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems</a></span><br>Fei Mi, Wanhao Zhou, <strong>Lingjing Kong</strong>, Fengyu Cai, Minlie Huang & Boi Faltings.<br><em>EMNLP 2021</em></p><p><span style=color:#006400><a href=https://openaccess.thecvf.com/content_CVPRW_2020/papers/w15/Mi_Generalized_Class_Incremental_Learning_CVPRW_2020_paper.pdf target=_blank>Generalized Class Incremental Learning</a></span><br>Fei Mi*, <strong>Lingjing Kong</strong>*, Tao Lin, Kaicheng Yu & Boi Faltings.<br><em>CVPR 2020 Workshop</em></p><p><span style=color:#006400><a href=https://ieeexplore.ieee.org/document/8647711 target=_blank>Power Consumption Evaluation in High Speed Visible Light Communication Systems</a></span><br><strong>Lingjing Kong</strong>, Cheng Chen, Yunlu Wang & Harald Hass.<br><em>IEEE Globecom Conference 2018</em></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://Lingjing-Kong.github.io/>Lingjing Kong</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/pmichaillat/hugo-website/ rel=noopener target=_blank>a modified version</a>
of
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>